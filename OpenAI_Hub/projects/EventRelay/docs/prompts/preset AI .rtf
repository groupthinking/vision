{\rtf1\ansi\ansicpg1252\cocoartf2709
\cocoatextscaling0\cocoaplatform0{\fonttbl\f0\fnil\fcharset0 HelveticaNeue;\f1\fnil\fcharset0 .SFNS-Regular_wdth_opsz110000_GRAD_wght2580000;\f2\fnil\fcharset0 HelveticaNeue-Italic;
}
{\colortbl;\red255\green255\blue255;\red236\green236\blue236;\red10\green10\blue10;\red236\green236\blue236;
}
{\*\expandedcolortbl;;\cssrgb\c94118\c94118\c94118\c90196;\cssrgb\c3922\c3922\c3922;\cssrgb\c94118\c94118\c94118;
}
{\*\listtable{\list\listtemplateid1\listhybrid{\listlevel\levelnfc23\levelnfcn23\leveljc0\leveljcn0\levelfollow0\levelstartat1\levelspace360\levelindent0{\*\levelmarker \{disc\}}{\leveltext\leveltemplateid1\'01\uc0\u8226 ;}{\levelnumbers;}\fi-360\li720\lin720 }{\listname ;}\listid1}
{\list\listtemplateid2\listhybrid{\listlevel\levelnfc23\levelnfcn23\leveljc0\leveljcn0\levelfollow0\levelstartat1\levelspace360\levelindent0{\*\levelmarker \{disc\}}{\leveltext\leveltemplateid101\'01\uc0\u8226 ;}{\levelnumbers;}\fi-360\li720\lin720 }{\listname ;}\listid2}}
{\*\listoverridetable{\listoverride\listid1\listoverridecount0\ls1}{\listoverride\listid2\listoverridecount0\ls2}}
\margl1440\margr1440\vieww11520\viewh8400\viewkind0
\deftab720
\pard\pardeftab720\partightenfactor0

\f0\fs32 \cf2 \cb3 \expnd0\expndtw0\kerning0
\outl0\strokewidth0 \strokec2 Baseline retrieval with cosine similarity, HyDE, Fine-Tuning, Chunking strategies, Reranking, Classification steps, Prompt engineering, Tool use, and query expansion.\
\
\pard\tx220\tx720\pardeftab720\li720\fi-720\sa160\partightenfactor0
\ls1\ilvl0
\f1\b \cf4 \cb3 \kerning1\expnd0\expndtw0 \outl0\strokewidth0 {\listtext	\uc0\u8226 	}\expnd0\expndtw0\kerning0
\outl0\strokewidth0 \strokec4 Prompt caching and cache hit
\f0\b0 \cf2 \strokec2 \'a0- Reusing commonly used prompts reduces the time to first token\
\pard\pardeftab720\partightenfactor0

\f1\b \cf4 \cb3 \strokec4 Prompt caching
\f0\b0 \cf2 \cb3 \strokec2 \'a0- This is a no brainer if you re-use prompts often (aka everyone). OpenAI will look for the first\'a0
\f2\i \cf2 \cb3 \strokec2 different
\f0\i0 \cf2 \cb3 \strokec2 \'a0token. Put your static system prompt at the beginning. If 1 token is different, you'll get a cache miss. The cache life is 5-10 minutes. However, no matter what, the cache always clears every hour.\
\pard\pardeftab720\partightenfactor0
\cf2 \cb3 \strokec2 \
\pard\tx720\pardeftab720\sa160\partightenfactor0
\cf2 \cb1 \strokec2 \
\pard\pardeftab720\partightenfactor0

\f1\b\fs30 \cf4 \strokec4 \
\pard\pardeftab720\partightenfactor0

\f0\b0\fs32 \cf2 \cb3 \strokec2 Ask the model to give you the bare minimum information that you actually need. Prompt length matters. Longer prompts = higher latency\
\
\pard\tx220\tx720\pardeftab720\li720\fi-720\sa160\partightenfactor0
\ls2\ilvl0
\f1\b \cf4 \cb3 \kerning1\expnd0\expndtw0 \outl0\strokewidth0 {\listtext	\uc0\u8226 	}\expnd0\expndtw0\kerning0
\outl0\strokewidth0 \strokec4 Batch API
\f0\b0 \cf2 \strokec2 \'a0- By running your requests async, you can save 50% on latency costs. Create a batch files to create a large number of requests. It goes quicker at non-peak times.\cb1 \
}