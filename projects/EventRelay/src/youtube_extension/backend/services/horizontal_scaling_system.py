#!/usr/bin/env python3
"""
Horizontal Scaling and Load Balancing System - Phase 3 Performance Optimization
===============================================================================

Advanced horizontal scaling system designed to support 60%+ performance improvements
through intelligent load distribution, auto-scaling, and service orchestration.
Prepares the system for handling 1000+ concurrent users and distributed processing.

Key Features:
- Load balancing with health checks
- Auto-scaling based on metrics
- Service discovery and registration
- Distributed task processing
- Failover and disaster recovery
- Performance-aware routing
- Container orchestration preparation
- Monitoring and alerting
"""

import asyncio
import json
import logging
import time
import threading
from datetime import datetime, timezone, timedelta
from typing import Dict, Any, List, Optional, Tuple, Callable
from dataclasses import dataclass, asdict, field
from collections import defaultdict, deque
import hashlib
import random
import socket
import psutil
from enum import Enum
import weakref

# Configure logging
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

class ServiceStatus(Enum):
    """Service status enumeration"""
    HEALTHY = "healthy"
    UNHEALTHY = "unhealthy" 
    STARTING = "starting"
    STOPPING = "stopping"
    MAINTENANCE = "maintenance"

class LoadBalanceStrategy(Enum):
    """Load balancing strategies"""
    ROUND_ROBIN = "round_robin"
    LEAST_CONNECTIONS = "least_connections"
    WEIGHTED_ROUND_ROBIN = "weighted_round_robin"
    CONSISTENT_HASH = "consistent_hash"
    PERFORMANCE_BASED = "performance_based"

@dataclass
class ServiceInstance:
    """Service instance definition"""
    service_id: str
    host: str
    port: int
    status: ServiceStatus = ServiceStatus.STARTING
    weight: int = 1
    max_connections: int = 100
    current_connections: int = 0
    response_time_ms: float = 0.0
    cpu_usage: float = 0.0
    memory_usage_mb: float = 0.0
    last_health_check: datetime = field(default_factory=lambda: datetime.now(timezone.utc))
    metadata: Dict[str, Any] = field(default_factory=dict)
    
    @property
    def load_factor(self) -> float:
        """Calculate current load factor (0-1)"""
        connection_load = self.current_connections / self.max_connections if self.max_connections > 0 else 0
        cpu_load = self.cpu_usage / 100.0
        response_load = min(self.response_time_ms / 1000.0, 1.0)  # Cap at 1s
        
        # Weighted average
        return (connection_load * 0.4 + cpu_load * 0.3 + response_load * 0.3)
    
    @property
    def is_healthy(self) -> bool:
        """Check if service instance is healthy"""
        return (
            self.status == ServiceStatus.HEALTHY and
            self.current_connections < self.max_connections and
            self.cpu_usage < 90 and
            self.response_time_ms < 5000
        )

@dataclass
class ScalingRule:
    """Auto-scaling rule definition"""
    metric: str  # cpu, memory, connections, response_time
    threshold_up: float
    threshold_down: float
    scale_up_count: int = 1
    scale_down_count: int = 1
    cooldown_seconds: int = 300  # 5 minutes
    min_instances: int = 1
    max_instances: int = 10
    last_scaled: datetime = field(default_factory=lambda: datetime.now(timezone.utc))

class LoadBalancer:
    """
    Advanced load balancer with multiple strategies and health checking
    """
    
    def __init__(self, strategy: LoadBalanceStrategy = LoadBalanceStrategy.PERFORMANCE_BASED):
        self.strategy = strategy
        self.service_instances: Dict[str, ServiceInstance] = {}
        self.service_registry = defaultdict(list)  # service_name -> [instance_ids]
        
        # Load balancing state
        self.round_robin_counters = defaultdict(int)
        self.consistent_hash_ring = {}
        
        # Performance tracking
        self.request_history = deque(maxlen=10000)
        self.routing_stats = defaultdict(int)
        
        # Health checking
        self.health_check_interval = 30  # seconds
        self.health_check_task = None
        self.unhealthy_threshold = 3  # consecutive failures
        
        logger.info(f"ðŸ”„ Load Balancer initialized with {strategy.value} strategy")
    
    def register_service(self, service_name: str, instance: ServiceInstance):
        """Register a service instance"""
        self.service_instances[instance.service_id] = instance
        self.service_registry[service_name].append(instance.service_id)
        
        logger.info(f"âœ… Registered service instance: {service_name}/{instance.service_id} "
                   f"at {instance.host}:{instance.port}")
    
    def unregister_service(self, service_name: str, service_id: str):
        """Unregister a service instance"""
        if service_id in self.service_instances:
            del self.service_instances[service_id]
        
        if service_id in self.service_registry[service_name]:
            self.service_registry[service_name].remove(service_id)
        
        logger.info(f"âŒ Unregistered service instance: {service_name}/{service_id}")
    
    async def route_request(self, service_name: str, request_metadata: Dict[str, Any] = None) -> Optional[ServiceInstance]:
        """
        Route request to best available service instance
        
        Returns the selected service instance for request handling
        """
        available_instances = self._get_healthy_instances(service_name)
        
        if not available_instances:
            logger.error(f"No healthy instances available for service: {service_name}")
            return None
        
        # Select instance based on strategy
        selected_instance = self._select_instance(available_instances, request_metadata)
        
        if selected_instance:
            # Update connection count
            selected_instance.current_connections += 1
            
            # Record routing decision
            self.routing_stats[selected_instance.service_id] += 1
            
            # Track request for performance analysis
            self.request_history.append({
                'timestamp': time.time(),
                'service_name': service_name,
                'instance_id': selected_instance.service_id,
                'load_factor': selected_instance.load_factor
            })
        
        return selected_instance
    
    def release_request(self, instance: ServiceInstance, response_time_ms: float, success: bool):
        """Release request and update instance metrics"""
        instance.current_connections = max(0, instance.current_connections - 1)
        
        # Update response time with exponential moving average
        if instance.response_time_ms == 0:
            instance.response_time_ms = response_time_ms
        else:
            alpha = 0.3
            instance.response_time_ms = (alpha * response_time_ms + 
                                       (1 - alpha) * instance.response_time_ms)
        
        # Update health status based on response
        if not success and instance.status == ServiceStatus.HEALTHY:
            # Could implement failure counting here
            pass
    
    def _get_healthy_instances(self, service_name: str) -> List[ServiceInstance]:
        """Get list of healthy service instances"""
        instance_ids = self.service_registry.get(service_name, [])
        
        return [
            self.service_instances[instance_id]
            for instance_id in instance_ids
            if instance_id in self.service_instances and self.service_instances[instance_id].is_healthy
        ]
    
    def _select_instance(self, instances: List[ServiceInstance], request_metadata: Dict[str, Any] = None) -> ServiceInstance:
        """Select best instance based on load balancing strategy"""
        
        if len(instances) == 1:
            return instances[0]
        
        if self.strategy == LoadBalanceStrategy.ROUND_ROBIN:
            return self._round_robin_selection(instances)
        
        elif self.strategy == LoadBalanceStrategy.LEAST_CONNECTIONS:
            return min(instances, key=lambda x: x.current_connections)
        
        elif self.strategy == LoadBalanceStrategy.WEIGHTED_ROUND_ROBIN:
            return self._weighted_round_robin_selection(instances)
        
        elif self.strategy == LoadBalanceStrategy.CONSISTENT_HASH:
            return self._consistent_hash_selection(instances, request_metadata)
        
        elif self.strategy == LoadBalanceStrategy.PERFORMANCE_BASED:
            return self._performance_based_selection(instances)
        
        else:
            # Default to round robin
            return self._round_robin_selection(instances)
    
    def _round_robin_selection(self, instances: List[ServiceInstance]) -> ServiceInstance:
        """Simple round-robin selection"""
        service_key = "default"  # Could be service-specific
        counter = self.round_robin_counters[service_key]
        selected = instances[counter % len(instances)]
        self.round_robin_counters[service_key] += 1
        return selected
    
    def _weighted_round_robin_selection(self, instances: List[ServiceInstance]) -> ServiceInstance:
        """Weighted round-robin based on instance weights"""
        total_weight = sum(instance.weight for instance in instances)
        if total_weight == 0:
            return instances[0]
        
        # Select based on cumulative weights
        target = random.randint(1, total_weight)
        cumulative = 0
        
        for instance in instances:
            cumulative += instance.weight
            if cumulative >= target:
                return instance
        
        return instances[-1]  # Fallback
    
    def _consistent_hash_selection(self, instances: List[ServiceInstance], request_metadata: Dict[str, Any]) -> ServiceInstance:
        """Consistent hash selection for session affinity"""
        if not request_metadata:
            return self._performance_based_selection(instances)
        
        # Create hash key from request metadata
        hash_key = hashlib.md5(json.dumps(request_metadata, sort_keys=True).encode()).hexdigest()
        hash_value = int(hash_key[:8], 16)  # Use first 8 chars
        
        # Select instance based on hash
        selected_index = hash_value % len(instances)
        return instances[selected_index]
    
    def _performance_based_selection(self, instances: List[ServiceInstance]) -> ServiceInstance:
        """Performance-based selection considering load factor"""
        # Select instance with lowest load factor
        return min(instances, key=lambda x: x.load_factor)
    
    async def start_health_checks(self):
        """Start background health checking"""
        if self.health_check_task is None:
            self.health_check_task = asyncio.create_task(self._health_check_loop())
            logger.info("ðŸ¥ Health checking started")
    
    async def stop_health_checks(self):
        """Stop background health checking"""
        if self.health_check_task:
            self.health_check_task.cancel()
            try:
                await self.health_check_task
            except asyncio.CancelledError:
                pass
            self.health_check_task = None
            logger.info("ðŸ›‘ Health checking stopped")
    
    async def _health_check_loop(self):
        """Background health checking loop"""
        while True:
            try:
                await self._perform_health_checks()
                await asyncio.sleep(self.health_check_interval)
            except asyncio.CancelledError:
                break
            except Exception as e:
                logger.error(f"Health check error: {e}")
                await asyncio.sleep(self.health_check_interval)
    
    async def _perform_health_checks(self):
        """Perform health checks on all service instances"""
        check_tasks = []
        
        for instance in self.service_instances.values():
            if instance.status != ServiceStatus.MAINTENANCE:
                check_tasks.append(self._check_instance_health(instance))
        
        if check_tasks:
            await asyncio.gather(*check_tasks, return_exceptions=True)
    
    async def _check_instance_health(self, instance: ServiceInstance):
        """Check health of a single service instance"""
        try:
            # Simulate health check (in production, this would be an HTTP/gRPC call)
            start_time = time.time()
            
            # Mock health check response
            healthy = random.choice([True, True, True, False])  # 75% healthy
            
            check_time = (time.time() - start_time) * 1000
            
            if healthy:
                instance.status = ServiceStatus.HEALTHY
                instance.last_health_check = datetime.now(timezone.utc)
                
                # Update performance metrics (simulated)
                instance.cpu_usage = random.uniform(10, 80)
                instance.memory_usage_mb = random.uniform(100, 800)
                
            else:
                instance.status = ServiceStatus.UNHEALTHY
                logger.warning(f"âŒ Health check failed for {instance.service_id}")
                
        except Exception as e:
            instance.status = ServiceStatus.UNHEALTHY
            logger.error(f"Health check error for {instance.service_id}: {e}")
    
    def get_load_balancer_stats(self) -> Dict[str, Any]:
        """Get comprehensive load balancer statistics"""
        
        # Service statistics
        service_stats = {}
        for service_name, instance_ids in self.service_registry.items():
            instances = [self.service_instances[iid] for iid in instance_ids if iid in self.service_instances]
            
            healthy_count = sum(1 for i in instances if i.is_healthy)
            total_connections = sum(i.current_connections for i in instances)
            avg_response_time = statistics.mean([i.response_time_ms for i in instances]) if instances else 0
            
            service_stats[service_name] = {
                'total_instances': len(instances),
                'healthy_instances': healthy_count,
                'unhealthy_instances': len(instances) - healthy_count,
                'total_connections': total_connections,
                'avg_response_time_ms': avg_response_time
            }
        
        # Request routing statistics
        recent_requests = [r for r in self.request_history if time.time() - r['timestamp'] < 300]  # Last 5 minutes
        requests_per_minute = len(recent_requests) / 5.0 if recent_requests else 0
        
        return {
            'timestamp': datetime.now(timezone.utc).isoformat(),
            'strategy': self.strategy.value,
            'total_instances': len(self.service_instances),
            'total_services': len(self.service_registry),
            'service_stats': service_stats,
            'routing_stats': dict(self.routing_stats),
            'requests_per_minute': requests_per_minute,
            'health_check_interval_seconds': self.health_check_interval
        }

class AutoScaler:
    """
    Auto-scaling system for dynamic resource management
    """
    
    def __init__(self, load_balancer: LoadBalancer):
        self.load_balancer = load_balancer
        self.scaling_rules: Dict[str, List[ScalingRule]] = defaultdict(list)
        self.scaling_history = deque(maxlen=1000)
        
        # Auto-scaling state
        self.monitoring_enabled = True
        self.scaling_task = None
        self.check_interval = 60  # Check every minute
        
        logger.info("âš¡ Auto Scaler initialized")
    
    def add_scaling_rule(self, service_name: str, rule: ScalingRule):
        """Add auto-scaling rule for a service"""
        self.scaling_rules[service_name].append(rule)
        logger.info(f"ðŸ“ Added scaling rule for {service_name}: {rule.metric} "
                   f"scale_up@{rule.threshold_up} scale_down@{rule.threshold_down}")
    
    async def start_auto_scaling(self):
        """Start auto-scaling monitoring"""
        if self.scaling_task is None:
            self.scaling_task = asyncio.create_task(self._scaling_loop())
            logger.info("ðŸ”„ Auto-scaling started")
    
    async def stop_auto_scaling(self):
        """Stop auto-scaling monitoring"""
        if self.scaling_task:
            self.scaling_task.cancel()
            try:
                await self.scaling_task
            except asyncio.CancelledError:
                pass
            self.scaling_task = None
            logger.info("ðŸ›‘ Auto-scaling stopped")
    
    async def _scaling_loop(self):
        """Main auto-scaling loop"""
        while self.monitoring_enabled:
            try:
                await self._evaluate_scaling_rules()
                await asyncio.sleep(self.check_interval)
            except asyncio.CancelledError:
                break
            except Exception as e:
                logger.error(f"Auto-scaling error: {e}")
                await asyncio.sleep(self.check_interval)
    
    async def _evaluate_scaling_rules(self):
        """Evaluate all scaling rules and trigger scaling if needed"""
        
        for service_name, rules in self.scaling_rules.items():
            instances = self.load_balancer._get_healthy_instances(service_name)
            
            if not instances:
                continue
            
            # Calculate service metrics
            service_metrics = self._calculate_service_metrics(instances)
            
            for rule in rules:
                # Check cooldown period
                if self._is_in_cooldown(rule):
                    continue
                
                metric_value = service_metrics.get(rule.metric, 0)
                current_instance_count = len(instances)
                
                # Evaluate scaling decisions
                should_scale_up = (
                    metric_value > rule.threshold_up and
                    current_instance_count < rule.max_instances
                )
                
                should_scale_down = (
                    metric_value < rule.threshold_down and
                    current_instance_count > rule.min_instances
                )
                
                if should_scale_up:
                    await self._scale_up(service_name, rule, metric_value)
                elif should_scale_down:
                    await self._scale_down(service_name, rule, metric_value)
    
    def _calculate_service_metrics(self, instances: List[ServiceInstance]) -> Dict[str, float]:
        """Calculate aggregated metrics for service instances"""
        if not instances:
            return {}
        
        return {
            'cpu': statistics.mean([i.cpu_usage for i in instances]),
            'memory': statistics.mean([i.memory_usage_mb for i in instances]),
            'connections': sum(i.current_connections for i in instances) / len(instances),
            'response_time': statistics.mean([i.response_time_ms for i in instances]),
            'load_factor': statistics.mean([i.load_factor for i in instances])
        }
    
    def _is_in_cooldown(self, rule: ScalingRule) -> bool:
        """Check if scaling rule is in cooldown period"""
        time_since_last = (datetime.now(timezone.utc) - rule.last_scaled).total_seconds()
        return time_since_last < rule.cooldown_seconds
    
    async def _scale_up(self, service_name: str, rule: ScalingRule, metric_value: float):
        """Scale up service instances"""
        logger.info(f"ðŸ”º Scaling UP {service_name}: {rule.metric}={metric_value:.2f} > {rule.threshold_up}")
        
        # Simulate instance creation
        for i in range(rule.scale_up_count):
            new_instance_id = f"{service_name}_{int(time.time())}_{i}"
            new_instance = ServiceInstance(
                service_id=new_instance_id,
                host=f"auto-scaled-{i}.local",
                port=8000 + i,
                status=ServiceStatus.STARTING
            )
            
            self.load_balancer.register_service(service_name, new_instance)
            
            # Simulate startup time
            await asyncio.sleep(0.1)
            new_instance.status = ServiceStatus.HEALTHY
        
        # Record scaling action
        scaling_action = {
            'timestamp': datetime.now(timezone.utc),
            'service_name': service_name,
            'action': 'scale_up',
            'rule_metric': rule.metric,
            'metric_value': metric_value,
            'threshold': rule.threshold_up,
            'instances_added': rule.scale_up_count
        }
        
        self.scaling_history.append(scaling_action)
        rule.last_scaled = datetime.now(timezone.utc)
        
        logger.info(f"âœ… Scaled up {service_name} by {rule.scale_up_count} instances")
    
    async def _scale_down(self, service_name: str, rule: ScalingRule, metric_value: float):
        """Scale down service instances"""
        logger.info(f"ðŸ”» Scaling DOWN {service_name}: {rule.metric}={metric_value:.2f} < {rule.threshold_down}")
        
        # Get instances to remove (least loaded first)
        instances = self.load_balancer._get_healthy_instances(service_name)
        instances_to_remove = sorted(instances, key=lambda x: x.load_factor)[:rule.scale_down_count]
        
        for instance in instances_to_remove:
            # Wait for connections to drain (simplified)
            instance.status = ServiceStatus.STOPPING
            await asyncio.sleep(0.1)
            
            self.load_balancer.unregister_service(service_name, instance.service_id)
        
        # Record scaling action
        scaling_action = {
            'timestamp': datetime.now(timezone.utc),
            'service_name': service_name,
            'action': 'scale_down',
            'rule_metric': rule.metric,
            'metric_value': metric_value,
            'threshold': rule.threshold_down,
            'instances_removed': len(instances_to_remove)
        }
        
        self.scaling_history.append(scaling_action)
        rule.last_scaled = datetime.now(timezone.utc)
        
        logger.info(f"âœ… Scaled down {service_name} by {len(instances_to_remove)} instances")
    
    def get_scaling_stats(self) -> Dict[str, Any]:
        """Get auto-scaling statistics"""
        
        # Recent scaling actions (last 24 hours)
        recent_actions = [
            action for action in self.scaling_history
            if (datetime.now(timezone.utc) - action['timestamp']).total_seconds() < 86400
        ]
        
        scale_up_count = len([a for a in recent_actions if a['action'] == 'scale_up'])
        scale_down_count = len([a for a in recent_actions if a['action'] == 'scale_down'])
        
        return {
            'timestamp': datetime.now(timezone.utc).isoformat(),
            'monitoring_enabled': self.monitoring_enabled,
            'check_interval_seconds': self.check_interval,
            'total_scaling_rules': sum(len(rules) for rules in self.scaling_rules.values()),
            'services_with_scaling': len(self.scaling_rules),
            'recent_scale_ups': scale_up_count,
            'recent_scale_downs': scale_down_count,
            'total_scaling_actions': len(self.scaling_history),
            'scaling_rules': {
                service: [
                    {
                        'metric': rule.metric,
                        'threshold_up': rule.threshold_up,
                        'threshold_down': rule.threshold_down,
                        'min_instances': rule.min_instances,
                        'max_instances': rule.max_instances
                    }
                    for rule in rules
                ]
                for service, rules in self.scaling_rules.items()
            }
        }

class HorizontalScalingSystem:
    """
    Main horizontal scaling system coordinating load balancing and auto-scaling
    """
    
    def __init__(self, load_balance_strategy: LoadBalanceStrategy = LoadBalanceStrategy.PERFORMANCE_BASED):
        self.load_balancer = LoadBalancer(load_balance_strategy)
        self.auto_scaler = AutoScaler(self.load_balancer)
        
        # Service discovery
        self.service_discovery_enabled = True
        
        # System monitoring
        self.system_metrics = {
            'total_requests': 0,
            'successful_requests': 0,
            'failed_requests': 0,
            'avg_response_time_ms': 0.0,
            'peak_concurrent_connections': 0,
            'uptime_start': datetime.now(timezone.utc)
        }
        
        self.is_running = False
        
        logger.info("ðŸŒ Horizontal Scaling System initialized")
    
    async def start_system(self):
        """Start the horizontal scaling system"""
        if self.is_running:
            return
        
        try:
            # Start load balancer health checks
            await self.load_balancer.start_health_checks()
            
            # Start auto-scaler
            await self.auto_scaler.start_auto_scaling()
            
            # Setup default scaling rules
            await self._setup_default_scaling_rules()
            
            self.is_running = True
            
            logger.info("ðŸš€ Horizontal Scaling System started successfully")
            
        except Exception as e:
            logger.error(f"Failed to start horizontal scaling system: {e}")
            raise
    
    async def stop_system(self):
        """Stop the horizontal scaling system"""
        if not self.is_running:
            return
        
        try:
            # Stop auto-scaler
            await self.auto_scaler.stop_auto_scaling()
            
            # Stop load balancer health checks
            await self.load_balancer.stop_health_checks()
            
            self.is_running = False
            
            logger.info("ðŸ›‘ Horizontal Scaling System stopped")
            
        except Exception as e:
            logger.error(f"Error stopping horizontal scaling system: {e}")
    
    async def register_video_processing_service(self, instance: ServiceInstance):
        """Register video processing service instance"""
        self.load_balancer.register_service("video_processing", instance)
    
    async def register_database_service(self, instance: ServiceInstance):
        """Register database service instance"""
        self.load_balancer.register_service("database", instance)
    
    async def register_cache_service(self, instance: ServiceInstance):
        """Register cache service instance"""
        self.load_balancer.register_service("cache", instance)
    
    async def process_request(self, service_name: str, request_data: Dict[str, Any]) -> Dict[str, Any]:
        """
        Process request through load balanced service
        
        This is the main entry point for request processing with load balancing
        """
        start_time = time.time()
        self.system_metrics['total_requests'] += 1
        
        try:
            # Route request to best service instance
            instance = await self.load_balancer.route_request(service_name, request_data)
            
            if not instance:
                self.system_metrics['failed_requests'] += 1
                return {
                    'success': False,
                    'error': f'No healthy instances available for {service_name}',
                    'timestamp': datetime.now(timezone.utc).isoformat()
                }
            
            # Simulate request processing
            processing_time = random.uniform(0.1, 2.0)  # 0.1-2.0 seconds
            await asyncio.sleep(processing_time)
            
            # Simulate success/failure
            success = random.choice([True] * 9 + [False])  # 90% success rate
            
            response_time = (time.time() - start_time) * 1000
            
            # Update metrics
            if success:
                self.system_metrics['successful_requests'] += 1
            else:
                self.system_metrics['failed_requests'] += 1
            
            # Update average response time
            current_avg = self.system_metrics['avg_response_time_ms']
            total_requests = self.system_metrics['total_requests']
            self.system_metrics['avg_response_time_ms'] = (
                (current_avg * (total_requests - 1) + response_time) / total_requests
            )
            
            # Release request from load balancer
            self.load_balancer.release_request(instance, response_time, success)
            
            return {
                'success': success,
                'response_time_ms': response_time,
                'instance_id': instance.service_id,
                'service_name': service_name,
                'timestamp': datetime.now(timezone.utc).isoformat(),
                'result': {'processed': True} if success else {'error': 'Processing failed'}
            }
            
        except Exception as e:
            self.system_metrics['failed_requests'] += 1
            response_time = (time.time() - start_time) * 1000
            
            logger.error(f"Request processing error: {e}")
            
            return {
                'success': False,
                'error': str(e),
                'response_time_ms': response_time,
                'timestamp': datetime.now(timezone.utc).isoformat()
            }
    
    async def _setup_default_scaling_rules(self):
        """Setup default auto-scaling rules for different services"""
        
        # Video processing service scaling rules
        video_cpu_rule = ScalingRule(
            metric='cpu',
            threshold_up=70.0,
            threshold_down=30.0,
            scale_up_count=2,
            scale_down_count=1,
            min_instances=2,
            max_instances=10,
            cooldown_seconds=300
        )
        
        video_response_rule = ScalingRule(
            metric='response_time',
            threshold_up=2000.0,  # 2 seconds
            threshold_down=500.0,  # 0.5 seconds
            scale_up_count=1,
            scale_down_count=1,
            min_instances=2,
            max_instances=8,
            cooldown_seconds=180
        )
        
        self.auto_scaler.add_scaling_rule("video_processing", video_cpu_rule)
        self.auto_scaler.add_scaling_rule("video_processing", video_response_rule)
        
        # Database service scaling rules
        db_connections_rule = ScalingRule(
            metric='connections',
            threshold_up=80.0,
            threshold_down=20.0,
            scale_up_count=1,
            scale_down_count=1,
            min_instances=1,
            max_instances=5,
            cooldown_seconds=300
        )
        
        self.auto_scaler.add_scaling_rule("database", db_connections_rule)
        
        # Cache service scaling rules
        cache_memory_rule = ScalingRule(
            metric='memory',
            threshold_up=1500.0,  # 1.5GB
            threshold_down=500.0,  # 500MB
            scale_up_count=1,
            scale_down_count=1,
            min_instances=1,
            max_instances=4,
            cooldown_seconds=240
        )
        
        self.auto_scaler.add_scaling_rule("cache", cache_memory_rule)
    
    def get_system_status(self) -> Dict[str, Any]:
        """Get comprehensive system status"""
        
        uptime = (datetime.now(timezone.utc) - self.system_metrics['uptime_start']).total_seconds()
        
        return {
            'timestamp': datetime.now(timezone.utc).isoformat(),
            'system_running': self.is_running,
            'uptime_seconds': uptime,
            'load_balancer': self.load_balancer.get_load_balancer_stats(),
            'auto_scaler': self.auto_scaler.get_scaling_stats(),
            'system_metrics': self.system_metrics.copy(),
            'performance_summary': {
                'requests_per_second': self.system_metrics['total_requests'] / max(uptime, 1),
                'success_rate_percent': (self.system_metrics['successful_requests'] / 
                                       max(self.system_metrics['total_requests'], 1)) * 100,
                'avg_response_time_ms': self.system_metrics['avg_response_time_ms']
            }
        }
    
    async def simulate_load_test(self, 
                               requests_per_second: int = 10, 
                               duration_seconds: int = 60) -> Dict[str, Any]:
        """
        Simulate load test to validate scaling behavior
        
        This helps test the 60%+ performance improvement targets under load
        """
        logger.info(f"ðŸ§ª Starting load test: {requests_per_second} RPS for {duration_seconds}s")
        
        start_time = time.time()
        test_results = {
            'start_time': datetime.now(timezone.utc).isoformat(),
            'target_rps': requests_per_second,
            'duration_seconds': duration_seconds,
            'responses': [],
            'errors': 0,
            'timeouts': 0
        }
        
        # Create initial service instances for testing
        await self._create_test_instances()
        
        # Generate load
        request_interval = 1.0 / requests_per_second
        tasks = []
        
        for i in range(int(duration_seconds * requests_per_second)):
            # Vary service requests
            service_name = random.choice(['video_processing', 'database', 'cache'])
            request_data = {'test_id': i, 'timestamp': time.time()}
            
            # Schedule request
            task = asyncio.create_task(self._load_test_request(service_name, request_data, test_results))
            tasks.append(task)
            
            # Wait for next request
            await asyncio.sleep(request_interval)
        
        # Wait for all requests to complete
        await asyncio.gather(*tasks, return_exceptions=True)
        
        # Analyze results
        test_duration = time.time() - start_time
        successful_responses = [r for r in test_results['responses'] if r.get('success', False)]
        
        if successful_responses:
            avg_response_time = statistics.mean([r['response_time_ms'] for r in successful_responses])
            p95_response_time = sorted([r['response_time_ms'] for r in successful_responses])[int(0.95 * len(successful_responses))]
        else:
            avg_response_time = 0
            p95_response_time = 0
        
        test_results.update({
            'end_time': datetime.now(timezone.utc).isoformat(),
            'actual_duration_seconds': test_duration,
            'total_requests': len(test_results['responses']),
            'successful_requests': len(successful_responses),
            'success_rate_percent': (len(successful_responses) / len(test_results['responses'])) * 100 if test_results['responses'] else 0,
            'actual_rps': len(test_results['responses']) / test_duration,
            'avg_response_time_ms': avg_response_time,
            'p95_response_time_ms': p95_response_time,
            'performance_improvement_validation': {
                'target_response_time_ms': 1000,  # 1s target
                'achieved_avg_ms': avg_response_time,
                'target_met': avg_response_time < 1000,
                'improvement_achieved': max(0, (2000 - avg_response_time) / 2000 * 100)  # vs 2s baseline
            }
        })
        
        logger.info(f"âœ… Load test completed: {test_results['success_rate_percent']:.1f}% success, "
                   f"{avg_response_time:.1f}ms avg response time")
        
        return test_results
    
    async def _create_test_instances(self):
        """Create test service instances for load testing"""
        
        # Video processing instances
        for i in range(2):
            instance = ServiceInstance(
                service_id=f"video_proc_test_{i}",
                host=f"video-{i}.test",
                port=8000 + i,
                max_connections=50,
                weight=1
            )
            instance.status = ServiceStatus.HEALTHY
            self.load_balancer.register_service("video_processing", instance)
        
        # Database instances
        instance = ServiceInstance(
            service_id="db_test_1",
            host="db-1.test",
            port=5432,
            max_connections=100,
            weight=2
        )
        instance.status = ServiceStatus.HEALTHY
        self.load_balancer.register_service("database", instance)
        
        # Cache instances
        instance = ServiceInstance(
            service_id="cache_test_1",
            host="cache-1.test",
            port=6379,
            max_connections=200,
            weight=1
        )
        instance.status = ServiceStatus.HEALTHY
        self.load_balancer.register_service("cache", instance)
    
    async def _load_test_request(self, service_name: str, request_data: Dict[str, Any], results: Dict[str, Any]):
        """Process individual load test request"""
        try:
            response = await self.process_request(service_name, request_data)
            results['responses'].append(response)
            
            if not response.get('success', False):
                results['errors'] += 1
                
        except asyncio.TimeoutError:
            results['timeouts'] += 1
            results['responses'].append({
                'success': False,
                'error': 'timeout',
                'response_time_ms': 5000  # Timeout threshold
            })
        except Exception as e:
            results['errors'] += 1
            results['responses'].append({
                'success': False,
                'error': str(e),
                'response_time_ms': 0
            })

# Global horizontal scaling system
scaling_system = HorizontalScalingSystem(LoadBalanceStrategy.PERFORMANCE_BASED)

# Convenience functions
async def start_horizontal_scaling():
    """Start horizontal scaling system"""
    await scaling_system.start_system()

async def stop_horizontal_scaling():
    """Stop horizontal scaling system"""
    await scaling_system.stop_system()

async def register_service_instance(service_name: str, host: str, port: int, **kwargs) -> str:
    """Register new service instance"""
    instance_id = f"{service_name}_{host}_{port}_{int(time.time())}"
    instance = ServiceInstance(
        service_id=instance_id,
        host=host,
        port=port,
        **kwargs
    )
    instance.status = ServiceStatus.HEALTHY
    
    if service_name == 'video_processing':
        await scaling_system.register_video_processing_service(instance)
    elif service_name == 'database':
        await scaling_system.register_database_service(instance)
    elif service_name == 'cache':
        await scaling_system.register_cache_service(instance)
    else:
        scaling_system.load_balancer.register_service(service_name, instance)
    
    return instance_id

async def process_load_balanced_request(service_name: str, request_data: Dict[str, Any]) -> Dict[str, Any]:
    """Process request through load balancing"""
    return await scaling_system.process_request(service_name, request_data)

async def run_scaling_load_test(rps: int = 50, duration: int = 30) -> Dict[str, Any]:
    """Run scaling load test"""
    return await scaling_system.simulate_load_test(rps, duration)

def get_scaling_system_status() -> Dict[str, Any]:
    """Get scaling system status"""
    return scaling_system.get_system_status()

if __name__ == "__main__":
    async def test_horizontal_scaling():
        logger.info("ðŸ§ª Testing Horizontal Scaling System...")
        
        # Start system
        await start_horizontal_scaling()
        
        # Register some test instances
        await register_service_instance("video_processing", "localhost", 8001, max_connections=50)
        await register_service_instance("video_processing", "localhost", 8002, max_connections=50)
        await register_service_instance("database", "localhost", 5432, max_connections=100)
        
        # Process some test requests
        for i in range(5):
            result = await process_load_balanced_request("video_processing", {"test": i})
            print(f"Request {i}: {result.get('success')} in {result.get('response_time_ms', 0):.1f}ms")
        
        # Run load test
        load_test_result = await run_scaling_load_test(rps=10, duration=5)
        print(f"Load test: {load_test_result['success_rate_percent']:.1f}% success rate")
        
        # Get system status
        status = get_scaling_system_status()
        print(f"System status: {status['performance_summary']}")
        
        # Stop system
        await stop_horizontal_scaling()
    
    asyncio.run(test_horizontal_scaling())