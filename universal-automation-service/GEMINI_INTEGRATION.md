# Gemini API Integration - Enhancement Plan

## üéØ Overview

Integrate Google Gemini 2.5 API to enhance YouTube video processing with direct video understanding capabilities, bypassing the need for YouTube Extension backend.

---

## üìä Gemini API Capabilities

### 1. Direct YouTube Video Processing

**Endpoint:** `generateContent`
**Models:** Gemini 2.5 Flash, Gemini 2.5 Pro

**Key Features:**
- ‚úÖ Pass YouTube URLs directly to API (no transcription service needed)
- ‚úÖ Video description and content extraction
- ‚úÖ Question answering about video content
- ‚úÖ Timestamp-specific references
- ‚úÖ Built-in audio transcription
- ‚úÖ Visual descriptions (frame analysis at 1 FPS)

**Limitations:**
- Free tier: 8 hours of YouTube video per day
- Only public videos supported
- Maximum 10 videos per request
- 1 frame per second sampling rate

### 2. URL Context Processing

**Tool:** `url_context` (available in `tools` parameter)
**Models:** Gemini 2.5 Pro/Flash

**Key Features:**
- ‚úÖ Process up to 20 URLs per request
- ‚úÖ Extract data from multiple sources
- ‚úÖ Compare documents across URLs
- ‚úÖ Synthesize content from multiple sources
- ‚úÖ Support for text, images, PDFs

**Supported Content Types:**
- HTML, JSON, plain text, XML, CSS
- Images: PNG, JPEG, BMP, WebP
- PDF documents

**Limitations:**
- 34MB maximum per URL
- No paywalled content
- No YouTube videos (use direct video API instead)
- No Google Workspace files
- No direct video/audio files

---

## üîß Integration Architecture

### Current Pipeline (EventRelay-based)
```
YouTube URL ‚Üí EventRelay (local backend)
    ‚îú‚îÄ‚îÄ youtube_api_proxy.py ‚Üí YouTube Data API v3
    ‚îú‚îÄ‚îÄ transcription_mcp_server.py ‚Üí Audio extraction + transcription
    ‚îú‚îÄ‚îÄ video_analysis_mcp_server.py ‚Üí Content analysis
    ‚îî‚îÄ‚îÄ learning_analytics_mcp_server.py ‚Üí Pattern extraction
    ‚Üì
UVAI Intelligence ‚Üí Executor
```

### Enhanced Pipeline (Gemini-powered)
```
YouTube URL ‚Üí Gemini API (direct video understanding)
    ‚îú‚îÄ‚îÄ generateContent with YouTube URL
    ‚îú‚îÄ‚îÄ Video summarization + transcription
    ‚îú‚îÄ‚îÄ Visual analysis (1 FPS frames)
    ‚îú‚îÄ‚îÄ Timestamp-specific insights
    ‚îî‚îÄ‚îÄ Content extraction
    ‚Üì
UVAI Intelligence ‚Üí Executor
```

### Hybrid Pipeline (Best of Both)
```
YouTube URL
    ‚Üì
Coordinator chooses processing mode:
    ‚îú‚îÄ‚îÄ Mode 1: Gemini API (fast, cloud-based, limited to 8hrs/day)
    ‚îÇ   ‚îî‚îÄ‚îÄ Direct video understanding
    ‚îú‚îÄ‚îÄ Mode 2: EventRelay (local, unlimited, requires backend)
    ‚îÇ   ‚îî‚îÄ‚îÄ Existing 4-server pipeline
    ‚îî‚îÄ‚îÄ Mode 3: Hybrid (Gemini + EventRelay)
        ‚îú‚îÄ‚îÄ Gemini: Video summarization + visual analysis
        ‚îî‚îÄ‚îÄ EventRelay: Detailed transcription + learning analytics
    ‚Üì
UVAI Intelligence ‚Üí Executor
```

---

## üíª Implementation Plan

### Phase 1: Gemini Video Processor (New Component)

**File:** `gemini_video_processor.py`

```python
#!/usr/bin/env python3
"""
Gemini API Video Processor
Direct YouTube video understanding using Gemini 2.5 API
"""
import os
from google import genai
from google.genai import types

class GeminiVideoProcessor:
    """Process YouTube videos using Gemini API"""

    def __init__(self):
        self.api_key = os.getenv("GEMINI_API_KEY")
        self.client = genai.Client(api_key=self.api_key)
        self.model = "gemini-2.5-flash"  # Fast and cost-effective

    def process_video(self, youtube_url: str) -> dict:
        """
        Process YouTube video with Gemini API
        Returns comprehensive video analysis
        """
        prompts = {
            "summary": "Summarize this video in 3 detailed sentences.",
            "transcript": "Provide a complete transcript with timestamps.",
            "topics": "List all main topics covered in this video.",
            "key_concepts": "Extract 5-10 key concepts or learnings.",
            "automation_opportunities": "Identify any processes or workflows shown that could be automated.",
            "visual_description": "Describe the key visual elements and demonstrations shown."
        }

        results = {}
        for task, prompt in prompts.items():
            response = self.client.models.generate_content(
                model=f'models/{self.model}',
                contents=types.Content(
                    parts=[
                        types.Part(
                            file_data=types.FileData(file_uri=youtube_url)
                        ),
                        types.Part(text=prompt)
                    ]
                )
            )
            results[task] = response.text

        return {
            "video_url": youtube_url,
            "processor": "gemini-2.5-flash",
            "analysis": results
        }
```

### Phase 2: URL Context Processor (Documentation Enhancement)

**File:** `gemini_url_context.py`

```python
#!/usr/bin/env python3
"""
Gemini URL Context Processor
Extract context from documentation URLs, code repos, etc.
"""
from google import genai
from google.genai.types import GenerateContentConfig

class GeminiURLContextProcessor:
    """Process multiple URLs for context extraction"""

    def __init__(self):
        self.client = genai.Client(api_key=os.getenv("GEMINI_API_KEY"))
        self.model = "gemini-2.5-flash"

    def process_urls(self, urls: list, query: str) -> dict:
        """
        Process multiple URLs with context extraction
        Use case: Extract skill patterns from multiple documentation sources
        """
        tools = [{"url_context": {}}]

        # Construct prompt with URLs
        prompt = f"{query}\n\nURLs to analyze:\n"
        for url in urls[:20]:  # Max 20 URLs
            prompt += f"- {url}\n"

        response = self.client.models.generate_content(
            model=f"models/{self.model}",
            contents=prompt,
            config=GenerateContentConfig(tools=tools)
        )

        return {
            "urls_processed": len(urls),
            "query": query,
            "extracted_context": response.text
        }
```

### Phase 3: Enhanced Coordinator (Mode Selection)

**Modifications to:** `coordinator.py`

```python
class UniversalAutomationCoordinator:
    def __init__(self, processing_mode="auto"):
        """
        processing_mode options:
        - "auto": Choose best mode based on availability
        - "gemini": Use Gemini API (requires GEMINI_API_KEY)
        - "eventrelay": Use EventRelay servers (requires local backend)
        - "hybrid": Use both for comprehensive analysis
        """
        self.processing_mode = processing_mode
        self.gemini_available = os.getenv("GEMINI_API_KEY") is not None
        self.eventrelay_available = self._check_eventrelay_backend()

    def process_youtube_url(self, youtube_url: str):
        # Stage 1: Choose processing mode
        if self.processing_mode == "auto":
            mode = self._select_best_mode()
        else:
            mode = self.processing_mode

        # Stage 2: Process video
        if mode == "gemini":
            video_data = self._process_with_gemini(youtube_url)
        elif mode == "eventrelay":
            video_data = self._process_with_eventrelay(youtube_url)
        else:  # hybrid
            video_data = self._process_hybrid(youtube_url)

        # Stage 3-4: UVAI + Executor (unchanged)
        ...
```

---

## üöÄ Enhancement Benefits

### Performance Improvements

| Aspect | Current (EventRelay) | With Gemini | Hybrid |
|--------|---------------------|-------------|--------|
| **Processing Speed** | 2-5 minutes | 30-60 seconds | 1-2 minutes |
| **Video Understanding** | Transcript only | Video + audio + visual | Comprehensive |
| **Setup Complexity** | Local backend required | API key only | Both |
| **Daily Limit** | Unlimited | 8 hours | 8 hours + unlimited |
| **Cost** | Free (self-hosted) | $0.0001/image | Mixed |

### New Capabilities

1. **Visual Analysis**
   - Frame-by-frame understanding (1 FPS)
   - Identify visual demonstrations
   - Screen recordings analysis
   - Code shown on screen extraction

2. **Faster Processing**
   - 3-5x faster than local transcription
   - No need to start YouTube Extension backend
   - Direct API access (cloud-based)

3. **Enhanced Intelligence**
   - Better context understanding (multimodal)
   - Timestamp-specific references
   - Visual + audio correlation
   - Improved automation opportunity detection

4. **Documentation Processing** (URL Context)
   - Process related documentation URLs
   - Compare multiple tutorials
   - Extract patterns from code repositories
   - Synthesize learnings from multiple sources

---

## üì¶ Implementation Steps

### Step 1: Add Gemini Dependencies

**File:** `requirements.txt`
```
google-genai>=0.2.0
google-auth>=2.23.0
```

Install:
```bash
pip3 install google-genai google-auth
```

### Step 2: Configure API Key

```bash
export GEMINI_API_KEY="your-gemini-api-key"
```

**Add to:** `config/pipeline_config.json`
```json
{
  "gemini": {
    "enabled": true,
    "api_key_env": "GEMINI_API_KEY",
    "model": "gemini-2.5-flash",
    "daily_video_limit": 8,
    "mode": "auto"
  }
}
```

### Step 3: Create Gemini Processor

```bash
cd /Users/garvey/Dev/OpenAI_Hub/universal-automation-service
touch gemini_video_processor.py
touch gemini_url_context.py
```

Implement classes as shown above.

### Step 4: Update Coordinator

Add Gemini processing mode to `coordinator.py`:

```python
def _process_with_gemini(self, youtube_url: str):
    from gemini_video_processor import GeminiVideoProcessor
    processor = GeminiVideoProcessor()
    gemini_result = processor.process_video(youtube_url)

    # Convert Gemini output to EventRelay format
    return self._convert_gemini_to_eventrelay_format(gemini_result)
```

### Step 5: Test Integration

```bash
# Test Gemini mode
python3 coordinator.py "https://youtube.com/watch?v=VIDEO_ID" --mode gemini

# Test hybrid mode
python3 coordinator.py "https://youtube.com/watch?v=VIDEO_ID" --mode hybrid
```

---

## üéØ Use Cases Enhanced by Gemini

### Use Case 1: Tutorial Video ‚Üí Skills
**Before (EventRelay only):**
- Extract transcript
- Analyze text for topics
- Generate skills from text patterns

**After (Gemini):**
- Visual understanding of demonstrations
- Identify code shown on screen
- Recognize UI interactions
- Extract step-by-step procedures from visual cues

**Result:** More accurate skill creation for visual tutorials

### Use Case 2: Coding Tutorial ‚Üí Automation
**Before:**
- Transcript: "First, we open the terminal and run this command..."
- Limited understanding of what command was shown

**After (Gemini):**
- Visual analysis: Detects actual command typed on screen
- Extracts exact syntax from video frames
- Identifies error messages shown
- Captures terminal output

**Result:** Precise code extraction, better automation workflows

### Use Case 3: Multi-Source Learning
**New capability with URL Context:**
- Process tutorial video (Gemini video API)
- Fetch related documentation (URL context API)
- Compare GitHub repo code (URL context API)
- Synthesize comprehensive skill from all sources

**Result:** More robust skills with documentation context

---

## üí∞ Cost Analysis

### Gemini API Pricing (as of documentation review)

**Video Processing:**
- Images (video frames): $0.0001 per image
- 1 FPS sampling = 60 frames per minute
- 5-minute video = 300 frames = $0.03

**Text Generation:**
- Input: $0.000035 per 1K characters
- Output: $0.00014 per 1K characters

**URL Context:**
- Included in text generation pricing
- Up to 34MB per URL

**Example Cost:**
- 10-minute YouTube video: ~$0.06
- With comprehensive prompts: ~$0.10 total
- 100 videos/day: ~$10/day

**Daily Limit (Free Tier):**
- 8 hours of video = 480 minutes
- At 1 FPS: 28,800 frames = $2.88 worth
- Free tier effectively provides significant processing capacity

---

## üîÑ Migration Strategy

### Phase 1: Add Gemini as Optional Enhancement
- Keep EventRelay as primary processor
- Add Gemini as optional enhancement (`--mode gemini`)
- Users choose based on availability

### Phase 2: Make Gemini Default (if API key present)
- Auto-detect GEMINI_API_KEY
- Use Gemini by default if available
- Fall back to EventRelay if not

### Phase 3: Hybrid Mode as Default
- Use Gemini for fast video understanding
- Use EventRelay for detailed analytics
- Combine results in UVAI intelligence layer

---

## üìä Performance Benchmarks (Estimated)

| Video Length | EventRelay | Gemini | Hybrid |
|--------------|-----------|--------|--------|
| 5 minutes | 2-3 min | 30 sec | 1 min |
| 10 minutes | 3-5 min | 45 sec | 1.5 min |
| 30 minutes | 10-15 min | 2 min | 5 min |
| 60 minutes | 20-30 min | 4 min | 10 min |

**Hybrid Advantage:**
- Gemini provides fast initial understanding
- EventRelay adds detailed analytics in parallel
- UVAI synthesizes both for comprehensive intelligence

---

## üîê Security Considerations

1. **API Key Storage**
   - Store in environment variable (not config files)
   - Never commit to repository
   - Use `.env` file for local development

2. **Rate Limiting**
   - Track daily video processing (8-hour limit)
   - Implement quota tracking
   - Graceful fallback to EventRelay when quota exceeded

3. **Data Privacy**
   - Videos sent to Google Cloud
   - Consider privacy for sensitive content
   - Option to keep processing local (EventRelay mode)

---

## üéâ Quick Start (Gemini Integration)

### 1. Get API Key
```bash
# Visit: https://aistudio.google.com/app/apikey
# Generate API key
export GEMINI_API_KEY="your-key-here"
```

### 2. Install Dependencies
```bash
pip3 install google-genai google-auth
```

### 3. Test Gemini Mode
```bash
python3 coordinator.py "https://youtube.com/watch?v=VIDEO_ID" --mode gemini
```

### 4. Enable Hybrid Mode
```bash
python3 coordinator.py "https://youtube.com/watch?v=VIDEO_ID" --mode hybrid
```

---

## üìà Expected Improvements

**Video Understanding:**
- +50% accuracy in visual demonstrations
- +75% faster processing
- +30% better automation opportunity detection

**Skill Generation:**
- More precise code extraction
- Better step-by-step procedure understanding
- Visual context for UI/UX automations

**User Experience:**
- No need to start local backend for quick processing
- Fallback options ensure reliability
- Choose mode based on privacy/speed requirements

---

## üöß Implementation Priority

**High Priority:**
1. ‚úÖ Gemini video processor (`gemini_video_processor.py`)
2. ‚úÖ Coordinator mode selection updates
3. ‚úÖ Configuration file updates
4. ‚úÖ Testing with real videos

**Medium Priority:**
5. URL context processor for documentation
6. Hybrid mode implementation
7. Quota tracking system
8. Performance benchmarking

**Low Priority:**
9. Dashboard UI updates (show processing mode)
10. Advanced visual analysis features
11. Multi-video batch processing with Gemini

---

## üìù Next Actions

1. **Create `gemini_video_processor.py`** with implementation from this document
2. **Update `coordinator.py`** to support `--mode` parameter
3. **Test with GEMINI_API_KEY** and real YouTube video
4. **Compare results:** Gemini vs EventRelay vs Hybrid
5. **Document performance** in PROJECT_SUMMARY.md

---

**Status:** Ready for implementation
**Estimated Implementation Time:** 2-3 hours
**Expected ROI:** 3-5x faster processing, enhanced video understanding, better skill generation

---

*This enhancement maintains backward compatibility while adding powerful cloud-based video understanding. Users can choose processing mode based on their requirements (speed vs privacy, cloud vs local).*
